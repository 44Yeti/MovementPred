#This script will generate a prediction algorithm for human movements based on the data generated by
#a magnetic sensor on each arm, each leg and the torso of the test person

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
#if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org") #RColorBrewer included
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org") #helps by LaTeX documents
#if(!require(abind)) install.packages("abind", repos = "http://cran.us.r-project.org")
if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org") #sub-packages in use: utils, stats, graphics

#Libraries needed for model calculations:
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org") #Penalized Multinomial Regression
if(!require(klaR)) install.packages("klaR", repos = "http://cran.us.r-project.org") #naives Bayes
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org") #Gradient Boosting

###################### Defining Functions for later use ########################################
# Usually I would use an extra script for functions but I understood our deliverable in a way
# that we only shall provide one script which has all code in it
################################################################################################

Features <- function(f){
  # Maximum/Minimum, mean, variance, skewness, kurtosis per column
  f_min <- apply(f,2,min)
  f_max <- apply(f,2,max)
  f_mean <- colMeans(f)
  f_var <- colVars(f)
  f_skew <- apply(f,2,skewness)
  f_kurt <- apply(f,2,kurtosis)
  
  #----------Discrete Fourier Transformation (with fast fourier transform)---------
  #Do Fast Fourier transformation (fft)
  f_DFT_temp <- apply(f,2,fft)
  
  #Get the peaks and the phases
  f_DFT_Mod_temp <- apply(f_DFT_temp,2,Mod)
  f_DFT_Arg_temp <- apply(f_DFT_temp,2,Arg)
  
  #Choose just half of the values (seconde half is like a mirror of the first)
  f_DFT_Mod_temp_Half <- f_DFT_Mod_temp[1:(length(f_DFT_Mod_temp[,1])/2+1),]
  f_DFT_Arg_temp_Half <- f_DFT_Arg_temp[1:(length(f_DFT_Arg_temp[,1])/2+1),]
  
  #Select the top 5 Peaks
  f_DFT_Mod <- apply(f_DFT_Mod_temp_Half,2,function(x){
    head(sort(x,decreasing = TRUE),5)
  })
  
  #Finde the corresponding phase/frequency value to the defined peaks - step 1: Get the index / step 2: get the values
  for(i in 1:45) {
    
    if(exists("Index_vec")==FALSE){ 
      Index_vec <- match(f_DFT_Mod[,i],f_DFT_Mod_temp_Half[,i])} 
    else{
      Index_vec <- c(Index_vec,match(f_DFT_Mod[,i],f_DFT_Mod_temp_Half[,i]))}
  }
  
  Index_matrix <- matrix(Index_vec, 5, 45)
  
  for(i in 1:45){
    if(exists("f_DFT_Arg")==FALSE){ 
      f_DFT_Arg <- f_DFT_Arg_temp_Half[Index_matrix[,i],i]}
    else{
      f_DFT_Arg <- c(f_DFT_Arg,f_DFT_Arg_temp_Half[Index_matrix[,i],i])}
  }
  
  #-------------------------------------- Autocorrelation -------------------
  #Get the Autocorrrelation for lag 50 (corresponds to 2 sek)
  f_Autoc_temp <- apply(f,2,function(x){
    acf(x, lag.max = 50, plot=FALSE)
  })
  
  #Get 11 values out of the 50 per "column" (here more "list section") - 
  #First value always =1 so not a lot of info from this, then I chose 4 correlation values
  #of the beginning, 4 values around 1 sek distance and 3 values at the end (=around 2sek difference)
  for(i in 1:45){
    if(exists("f_Autoc")==FALSE){
      f_Autoc <- f_Autoc_temp[[i]]$acf[c(2:5,23:26,48:50)]}
    else{
      f_Autoc <- c(f_Autoc,f_Autoc_temp[[i]]$acf[c(2:5,23:26,48:50)])}
  }
  #---------------------Building the Feature Vector ----------------------------------------------------------------------
  Feature_Vector <<- c(f_min,f_max,f_mean,f_var,f_skew,f_kurt,as.vector(f_DFT_Mod),f_DFT_Arg, f_Autoc) #global Variable "<<-"
}

########################## Create Feature_Matrix out of raw data #######################################################
#--------------Prepare ColNames and the variables to extract the files (activites, person, files)---------------
ColNames <- c("T_xacc", "T_yacc", "T_zacc", "T_xgyro", "T_ygyro", "T_zgyro", "T_xmag", "T_ymag", "T_zmag",
              "RA_xacc", "RA_yacc", "RA_zacc", "RA_xgyro", "RA_ygyro", "RA_zgyro", "RA_xmag", "RA_ymag", "RA_zmag",
              "LA_xacc", "LA_yacc", "LA_zacc", "LA_xgyro", "LA_ygyro", "LA_zgyro", "LA_xmag", "LA_ymag", "LA_zmag",
              "RL_xacc", "RL_yacc", "RL_zacc", "RL_xgyro", "RL_ygyro", "RL_zgyro", "RL_xmag", "RL_ymag", "RL_zmag",
              "LL_xacc", "LL_yacc", "LL_zacc", "LL_xgyro", "LL_ygyro", "LL_zgyro", "LL_xmag", "LL_ymag", "LL_zmag")

#19 folders for the 19 activities:
activityNames <- c("a01","a02", "a03", "a04", "a05", "a06", "a07", "a08", "a09", "a10", "a11", "a12", "a13", "a14", "a15", 
                   "a16", "a17", "a18", "a19")

#8 person folders in each activity folder:
personNumber <- c("p1","p2","p3","p4","p5","p6","p7","p8")

#60 file names in each person folder:
fileNames <- sprintf("%02d",seq(1:60)) # gives me all numbers als 2 digits so that 1, 2 is "01", "02" etc


#----------------Creating the Feature_Vector as base for the Feature_Matrix----------------------------------------------
#Download the raw data
td <- tempdir()
tf = tempfile(tmpdir=td, fileext=".zip")
download.file("https://publicdatarfu.blob.core.windows.net/publicdatarfucontainer/data.zip", tf)
unzip(tf, exdir=td)
#I had "\\" as separater on the path which did not work on my windows system, to correct this I found the
#following correction option, which I hope will solved in a way that my code is also running on a non windows
#system without any problems. In worst case please adjust the separaters in fpath manually...
fpath <- normalizePath(td, winslash = "/", mustWork = NA)

#Loop for the 19 activities
FV2 <- lapply(z <- activityNames, function(z){
  
  #Loop for the 8 person
  FV1 <- lapply(y <- personNumber, function(y){
    
    #Loop for the 60 time-series files
    FV <- lapply(x <- fileNames, function(x){
      file <- paste0(fpath,"/data/",z,"/",y,"/s",x,".txt")
      #file <- paste0("C:/Users/fure/OneDrive/Projects/R/TempRohDaten/data/",z,paste0("/",y,paste0("/s",x,".txt")))
      f <- as.matrix(read_delim(file, delim = ",", col_names = ColNames))
      if(exists("FV")==FALSE){
        FV <- vector("list",1)
        FV[[x]] <- Features(f)}
      else{
        FV[[x]] <- Features(f)}
    })
    
    if(exists("FV1")==FALSE){
      FV1 <- vector("list",1)
      FV1[[y]] <- Reduce(c,FV)}
    else{
      FV1[[y]] <- Reduce(c,FV)}
  })
  if(exists("FV2")==FALSE){
    FV2 <- vector("list",1)
    FV2[[z]] <- Reduce(c,FV1)}
  else{
    FV2[[z]] <- Reduce(c,FV1)}
})

Feature_Vector <- Reduce(c,FV2)

#--------------------Final step: Creat Feature_Matrix and clean-up---------------------------
#Prepare for the Naming of the columns and rows:
temp <- rep(ColNames,each=5)
temp1 <- c(rep(c("DFT_Mod_1_","DFT_Mod_2_","DFT_Mod_3_","DFT_Mod_4_","DFT_Mod_5_"),45))
temp_Mod <- paste0(temp1,temp)

temp1 <- c(rep(c("DFT_Arg_1_","DFT_Arg_2_","DFT_Arg_3_","DFT_Arg_4_","DFT_Arg_5_"),45))
temp_Arg <- paste0(temp1,temp)

temp <- rep(ColNames,each=11)
temp1 <- c(rep(c("DFT_Atuoc_1_","DFT_Atuoc_2_","DFT_Atuoc_3_","DFT_Atuoc_4_","DFT_Atuoc_5_","DFT_Atuoc_6_",
                 "DFT_Atuoc_7_","DFT_Atuoc_8_","DFT_Atuoc_9_","DFT_Atuoc_10_","DFT_Atuoc_11_"),45))
temp_Autoc <- paste0(temp1,temp)


DimNames_Col <- c(paste0("min_",ColNames),paste0("max_",ColNames),paste0("mean_",ColNames),paste0("var_",ColNames),
                  paste0("skew_",ColNames), paste0("kurt_",ColNames),temp_Mod,temp_Arg,temp_Autoc)


DimNames_Row <- c(rep("a01", (60*8)),rep("a02", (60*8)),rep("a03", (60*8)),rep("a04", (60*8)),rep("a05", (60*8)),
                  rep("a06", (60*8)),rep("a07", (60*8)),rep("a08", (60*8)),rep("a09", (60*8)),rep("a10", (60*8)),
                  rep("a11", (60*8)),rep("a12", (60*8)),rep("a13", (60*8)),rep("a14", (60*8)),rep("a15", (60*8)),
                  rep("a16", (60*8)),rep("a17", (60*8)),rep("a18", (60*8)),rep("a19", (60*8)))

#Generate a Matrix out of the Feature_Vector:
Feature_Matrix1 <- matrix(Feature_Vector,9120,1215,byrow = TRUE, dimnames = list(DimNames_Row, DimNames_Col))

#remove unnessecary objects
rm(x,y,z,activityNames,personNumber,fileNames,f,file,temp,temp1,temp_Arg,temp_Mod,temp_Autoc,FV2,Feature_Vector)

###################### Generate Train and Test Data ################################################################
#Split Feature dataset in a Feature_train and Feature_test set - use 15% of data for test set
set.seed(2020) #so that I have reproducable results

test_index <- createDataPartition(y = rownames(Feature_Matrix), times = 1, p = 0.15, list = FALSE)
Feature_train <- Feature_Matrix[-test_index,]
Feature_test <- Feature_Matrix[test_index,]

train_labels <- rownames(Feature_train)
test_labels <- rownames(Feature_test)

#remove unnessecary object
rm(test_index)

######################## Analyse Data from Feature Set #############################################################
#------------------Compare the values of the different columns to see if they have similar scale:----------
#First Index are the first Columns of each Feature-Typ (min, max, variance....kurtosis), for DFT_Mod/DFT_Arg
#the first value of each sensor and for Autocorrelation every 45th value:


#for(i in 1:27){
#  ifelse(exists("index")=="FALSE",index <- c(1),index <- c(index,1+45*i))
#}
#index <- replace(index,27,1215)
#index

#Move by one column higher compared to index for the DFT_Mod values
#index1 <- replace(index,c(6:10),c(272,317,362,407,452))
#index1

#Move by two columns higher compared to index for the DFT_Mod values
#index2 <- replace(index,c(6:10),c(273,318,363,408,453))
#index2

#Bring all indexes in a matrix:
#indexM <- matrix(c(index,index1,index2),27,3)
#indexM

#Generate three pictures to show how values differe to each other:
#apply(indexM,2,function(x){
#  image(as.matrix(Feature_Matrix[,x]), col = rev(RColorBrewer::brewer.pal(9, "RdBu")))
#})

########################## Feature Reduction #######################################################################

#Geprüft, ob dank "nearZero" gewisse Features verworfen werden können? [sind nur 2...]
#nzv <- nearZeroVar(Feature_Matrix)

#pca Analyse - erneut wichtiger Link: https://strata.uga.edu/software/pdf/pcaTutorial.pdf
#Zentriert wird es per Default, aber ich habe nun auch noch scale. auf TRUE gesetzt, so dass die Skalen vergleichbar
#werden zwischen den Featuren:
pca <- prcomp(Feature_train, scale. = FALSE)

#!!!!!! Hier PCA Variablen definieren: gemäss Seite 4 von Tutorial !!!!!#
variance <- (pca$sdev)^2
rotation <- pca$rotation 
rownames(rotation) <- colnames(Feature_Matrix)  
scores <- pca$x

#Cool Image but needs lot of time...
#d <- dist(Feature_Matrix)
#image(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, "RdBu")))

#Shows % of Variance explained by PC
varPercent <- variance/sum(variance) * 100
barplot(varPercent, xlab='PC', ylab='Percent Variance', names.arg=1:length(varPercent), las=1, col='gray')
abline(h=1/ncol(Feature_Matrix)*100, col='red')

#Find # of PC which explain more dann 99% of Variance
PC_99 <- match("TRUE",cumsum(variance/sum(variance))>=0.99)-1
PC_99

################################# Train Models ####################################################################################
####################################################################################################################################

#-----------------Preparation: Bring pca_train/pca_test set into tibbles and define a "fitControl"-------
#pca_train into tibbles incl. Acitivites as factor
pca_train <- scores[,1:PC_99]
pca_train <- pca_train %>% as_tibble() %>% cbind(train_labels,.) %>% rename(Activity = train_labels)
pca_train$Activity <- factor(pca_train$Activity)

#Transform test set 
pca_test <- sweep(Feature_test, 2, colMeans(Feature_test)) %*% rotation
pca_test <- pca_test[,1:PC_99]
pca_test <- pca_test %>% as_tibble() %>% cbind(test_labels,.) %>% rename(Activity = test_labels)
pca_test$Activity <- factor(pca_test$Activity)

#k-Fold CrossValidation with 5 folds and 2 repetitions:
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 2,
  verboseIter = TRUE)

#----------------------- Penalized Multinomial Regression -------------------------------------------
set.seed(2020)

Fit_multi <- train(Activity ~ ., data = pca_train, 
                 method = "multinom", 
                 trControl = fitControl,
                 tuneGrid = data.frame(decay = c(0,0.0001)))
Fit_multi

predictions <- predict(Fit_multi, pca_test)
CM_multi <- confusionMatrix(predictions, pca_test$Activity)
confusionMatrix(predictions, pca_test$Activity)$overall["Accuracy"]

#-----------------------k-Nearest Neighbors--------------------------------------------------
set.seed(2020)

Fit_knn <- train(Activity ~ ., data = pca_train, 
              method = "knn", 
              trControl = fitControl,
              tuneGrid = data.frame(k = c(5,7,9,11,15,20)))
Fit_knn

predictions <- predict(Fit_knn, pca_test)
CM_knn <- confusionMatrix(predictions, pca_test$Activity)
confusionMatrix(predictions, pca_test$Activity)$overall["Accuracy"]

#-----------------------naive Bayes -- lots of warnings because auf small values ----------------
set.seed(2020)

Fit_nBayes <- train(Activity ~ ., data = pca_train, 
                 method = "nb", 
                 trControl = fitControl,
                 tuneGrid = data.frame(fL = 0, usekernel = TRUE, adjust = 1))
Fit_nBayes

predictions <- predict(Fit_nBayes, pca_test)
CM_nBayes <- confusionMatrix(predictions, pca_test$Activity)
confusionMatrix(predictions, pca_test$Activity)$overall["Accuracy"]

#-----------------------Support Vector Machines----------------------------------------------------
set.seed(2020)

Fit_svm <- train(Activity ~ ., data = pca_train, 
                 method = "svmLinear2",
                 trControl = fitControl,
                tuneGrid = data.frame(cost = c(0.5,1)))
Fit_svm

predictions <- predict(Fit_svm, pca_test)
CM_svm <- confusionMatrix(predictions, pca_test$Activity)
confusionMatrix(predictions, pca_test$Activity)$overall["Accuracy"]

#-----------------------Gradient Boosting linear ----------------------------------------------------
set.seed(2020)

Fit_gb <- train(Activity ~ ., data = pca_train, 
                    method = "xgbLinear",
                    trControl = fitControl,
                    tuneGrid = data.frame(nrounds= 150, lambda=c(0,0.1), 
                                          alpha=0, eta=c(0.3,0.4)))
Fit_gb

predictions <- predict(Fit_gb, pca_test)
CM_gb <- confusionMatrix(predictions, pca_test$Activity)
confusionMatrix(predictions, pca_test$Activity)$overall["Accuracy"]

#-----------------------Gradient Boosting Tree ----------------------------------------------------
set.seed(2020)

len=3
gbtGrid <-  expand.grid(max_depth = seq(2, len),
                         nrounds = floor((2:len) * 50),
                         eta = .3,
                         gamma = 0,
                         colsample_bytree = c(.6, .8),
                         min_child_weight = c(1),
                         subsample = seq(.5, .75, length = len-1))

Fit_gbt <- train(Activity ~ ., data = pca_train, 
                method = "xgbTree",
                trControl = fitControl,
                tuneGrid = gbtGrid)#data.frame(nrounds= c(100,150),colsample_bytree=c(0.6,0.8), 
                                      #gamma=0, eta=0.3, min_child_weight=1, subsample=c(0.5,0.75),
                                      #max_depth=c(2,3)))
Fit_gbt

predictions <- predict(Fit_gbt, pca_test)
CM_gbt <- confusionMatrix(predictions, pca_test$Activity)
confusionMatrix(predictions, pca_test$Activity)$overall["Accuracy"]

#----------------------Compare all Accuracies----------------------------------
results <- tibble(method= c("Penalized Multinomial Regression", "k-Nearest Neighbors", "Naive Bayes",
                           "Support Vector Machines", "eXtreme Gradient Boosting (linear)", 
                           "eXtreme Gradient Boosting (Tree)"),
                                 Accuracy = c(CM_multi$overall["Accuracy"], CM_knn$overall["Accuracy"],
                                              CM_nBayes$overall["Accuracy"], CM_svm$overall["Accuracy"],
                                              CM_gb$overall["Accuracy"], CM_gbt$overall["Accuracy"]))

results <- results[order(results$Accuracy, decreasing = TRUE),]

results %>% knitr::kable()


##############################################################################################
##############################################################################################
###################################### END, END, END #########################################
##############################################################################################
##############################################################################################



#------------------------------------------------------------------------------


y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(data = y_hat_knn, reference = mnist_27$test$y)$overall["Accuracy"]


library(caret)
k <- 36
x_train <- pca$x[,1:k]
y <- factor(mnist$train$labels)
fit <- knn3(x_train, y)

x_test <- sweep(mnist$test$images, 2, col_means) %*% pca$rotation
x_test <- x_test[,1:k]

y_hat <- predict(fit, x_test, type = "class")
confusionMatrix(y_hat, factor(mnist$test$labels))$overall["Accuracy"]
#> Accuracy 
#>    0.975






########################### Make data ready to use  - Kann man löschen - davor noch schauen wegen zip-Datei...############################
########################### 

#----------------------------- Establish the basic tables out ot the raw data ---

#Define column names (legend: T=torso, RA=right arm, LA=left arm, RL=right leg, LL=Left leg / x,y,z = axis of direction /
#acc=accelerometer, gyro=gyroscope, mag=magnetometer)
ColNames <- c("T_xacc", "T_yacc", "T_zacc", "T_xgyro", "T_ygyro", "T_zgyro", "T_xmag", "T_ymag", "T_zmag",
              "RA_xacc", "RA_yacc", "RA_zacc", "RA_xgyro", "RA_ygyro", "RA_zgyro", "RA_xmag", "RA_ymag", "RA_zmag",
              "LA_xacc", "LA_yacc", "LA_zacc", "LA_xgyro", "LA_ygyro", "LA_zgyro", "LA_xmag", "LA_ymag", "LA_zmag",
              "RL_xacc", "RL_yacc", "RL_zacc", "RL_xgyro", "RL_ygyro", "RL_zgyro", "RL_xmag", "RL_ymag", "RL_zmag",
              "LL_xacc", "LL_yacc", "LL_zacc", "LL_xgyro", "LL_ygyro", "LL_zgyro", "LL_xmag", "LL_ymag", "LL_zmag")

#um schneller rechnen zu können anziehen des Files aus lokalem Verzeichnis
#temp <- "C:/Users/fure/OneDrive/Projects/R/TempRohDaten/data/a01/p1/s01.txt"
#a1 <- read_delim(temp, delim = ",", col_names = ColNames)

#for(y in personNumber){
#  for(x in fileNames){
#    file <- paste0("C:/Users/fure/OneDrive/Projects/R/TempRohDaten/data/a01/p",y,paste0("/s",x,".txt"))
#    a1 <- cbind(a1, read_delim(file, delim = ",", col_names = ColNames))
#    a1
#  }
#}

#Download the zip File with the raw data
dl <- tempfile()
download.file("https://publicdatarfu.blob.core.windows.net/publicdatarfucontainer/data.zip", dl)
#!!!!!! Prüfen, ob es auch tempfolder gibt, damit man alles unzipen kann...

#Generate the basic table for activity nb 1 just with one file:
temp <- unzip(dl, "data/a01/p1/s01.txt")
a1 <- read_delim(temp, delim = ",", col_names = ColNames)

#Build file a1 --> all measurements from all person for activity nb 1:
fileNames <- sprintf("%02d",seq(1:60))
personNumber <- seq(1:8)

for(y in personNumber){
  for(x in fileNames){
    file <- unzip(dl,paste0("data/a01/p",y,paste0("/s",x,".txt")))
    a1 <- cbind(a1, read_delim(file, delim = ",", col_names = ColNames))
    a1
  }
}
identical(a1[,1:45], a1[,46:90]) #first file is a double count because of the choicen approach
a1[1:45] <- list(NULL) #delete the frist file (first 45 columns)
#a1
dim(a1)