---
title: "Movement Prediction Project"
author: "44Yeti"
date: "10 5 2020"
output: 
  pdf_document:
    toc: true
    toc_depth: 4
    fig_caption: true
    keep_tex: true
    number_sections: true
header-includes:
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, fig.pos = "H")
```

\pagebreak
## Executive Summary
This is the final challenge in the "HarvardX Professional Certificate in Data Science" course. For this project we had the task to apply machine learning techniques which go beyond the standard linear regression we applied in the Movielens project. Even so we shall use a dataset which we not used already in the previous courses (movielens, mnist, iris...). As potential soucres *[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)*[^1] or *[Kaggle](https://www.kaggle.com/datasets)*[^2] were recommended.
After making me some thoughts about the assignment I defined the following criteria which the challenge shall provide to me:

1. The raw data has to be split up in several files, so there is a challenge to bring all this single pieces togehter in one usable dataset
2. In raw data the features shall not be pre-processed in a way that they are already "ready for use", so some pre-processing has to be done by me
3. The machine learning challenge shall be a classification challenge with more than two features

I have defined these challenges as they are topics we did not touch too much during the courses and I therefore see the need for some practise. Especially the first two point were important to me as they seem to me skills which will be needed quite often in real life cases.
After some searching on the UCI and Kaggle platform I found the *[Daily and Sports Activities Data Set](https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities)*[^3] on UCI. 
For the creation of this dataset 8 person had to fulfil 19 different activites, while their movements got measurent by senors.The raw data is stored in 60 different files (each file consists out of a 125 * 45 matrix) per person per activity, which means that we have to handle $60*8*19=9120$ different files. Even so the features for the machine learning model had to be calculated from the raw data - the raw data did not provide "usable" features directly. And as the machine learning challenge was a multi-variable classification problem all requirements I set for my project challenge were given with this dataset and so my choice was done.
During my project the articel "Comparative study on classifying human activities with miniature inertial and magnetic sensors" from Kerem Altun and Bilur Barshan[^4] was a good support, as they used the same dataset for their studies. Their articel was especially helpful by the definition of the different features, as I am not exactly a subject matter expert in the sensor-data world. Even so it helped me to set an Accuracy target for my machine learning model: They reached with their most successful model an Accuracy of over 99%, so this was my goal too.
To make it a bit harder I skipped most of the models they applied including their most successful one (Bayesian decision making (BDM)) for my work. My set of models consits out of the following six algorithms: Penalized Multinomial Regression (PMR), k-Nearest Neighbors (k-NN), naive Bayes (nBayes), Support Vector Machines (SVM), Gradient Boosting linear (GBl) and Gradient Boosting tree (GBt). Beside the k-NN algorithm these were all models not used by the study from Altun and Barshan.
The best result I achieved with the Gradient Boosting tree algorithm with an Accuracy of **99,1%** and so I achieved the goal I set.
To get this result I followed the following steps: 

1. Analyse and understand the raw dataset
2. Load the different raw data files, transform them into a feature set and generate a "Feature-Vector" as our base for all next steps
3. Generate a train- and test dataset
4. Analyse the train dataset and do a feature reduction
5. Train all six models with the train dataset and check its accuracy with the test dataset
6. Analyse and discuss the final results

Along these steps I have also done the structure of this document plus I added a chapter about my personal conclusion and potential future work at the end.

## The raw data dataset
The raw dataset one can find under following link: https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities. 
To generate this data 8 people (4 women, 4 men) got equipped with 5 sensor units each. The sensor units were positioned at the torso (T), right arm (RA), left arm (LA), right leg (RL) and left leg (LL).
Each of this sensor units included three different sensors, of which each delivered values for the x-, y- and z-axis. Therefore for all measure points in time we get three values from the accelerometer- (xacc, yacc, zacc), gyroscope- (xgyro, ygyro, zgyro) and magnetometer-sensors (xmag, ymag, zmag) each. This is how such a sensor unit is looking like:
```{r SensorPicture, echo=FALSE, fig.cap="MTx 3-DOF orientation tracker (reprinted from http://www.xsens.com/en/general/mtx)"}
SensorPic= "https://publicdatarfu.blob.core.windows.net/publicdatarfucontainer/SensorUnit.png"
download.file(SensorPic,'SensorPic.png', mode = 'wb')
knitr::include_graphics("SensorPic.png")
```
   
As the measurements get done in a pace of 25 Hertz this means every second we get the result of 25 measure points in time and as each measure point includes 45 values (5 sensor units * 3 sensors * 3 axis) all second we get $25 * 45 = 1125$ values. In the dataset this data got captured as follows: per each 5 second they generated a txt-file in which the values are stored as a 125 * 45 matrix (125 rows as we have 25 * 5 "capture points"). The build-up logic of the files is allways the same and follows the following pattern:
Columns 1-45 correspond to:
   
T_xacc, T_yacc, T_zacc, T_xgyro, ..., T_ymag, T_zmag, RA_xacc, RA_yacc, RA_zacc, RA_xgyro, ..., RA_ymag, RA_zmag, LA_xacc, LA_yacc, LA_zacc, LA_xgyro, ..., LA_ymag, LA_zmag, RL_xacc, RL_yacc, RL_zacc, RL_xgyro, ..., RL_ymag, RL_zmag, LL_xacc, LL_yacc, LL_zacc, LL_xgyro, ..., LL_ymag, LL_zmag
   
Therefore:

* columns 1-9 correspond to the sensors in unit 1 (T)
* columns 10-18 correspond to the sensors in unit 2 (RA)
* columns 19-27 correspond to the sensors in unit 3 (LA)
* columns 28-36 correspond to the sensors in unit 4 (RL)
* columns 37-45 correspond to the sensors in unit 5 (LL) 
   
Now as we know how the single input files are structured let us zoom out and have a look at the activities the participants were measured at. 
Each participant had to performe the following 19 activites for exact 5 minutes:

* sitting (A1)
* standing (A2)
* lying on back and on right side (A3 and A4)
* ascending and descending stairs (A5 and A6)
* standing in an elevator still (A7)
* and moving around in an elevator (A8)
* walking in a parking lot (A9)
* walking on a treadmill with a speed of 4 km/h (in flat and 15 deg inclined positions) (A10 and A11)
* running on a treadmill with a speed of 8 km/h (A12)
* exercising on a stepper (A13)
* exercising on a cross trainer (A14)
* cycling on an exercise bike in horizontal and vertical positions (A15 and A16)
* rowing (A17)
* jumping (A18)
* and playing basketball (A19)
  
In addtion it is to say that the subjects did not get any guidance how to performe the activities so it is likely that we will see some "inter-subject" variations.

So now we have all components together and can have a look how the dataset is delivered. First of all it is a zip.file called "data.zip". Within this zip.file we finde a folder per activity (folder a01 till a19). Within each of this folders there is a folder per subject (folder p1 till p8). And finally within this subject folders are always 60 input files (files s01.txt till s60.txt) - explanation: 5 seconds are captured in one file, each activity get measured for 5min equals 60 files.
In a classical Windows Explorer view that looks like this:
```{r Datastructure, echo=FALSE, fig.cap="Visualisation of data structure", fig.align='center'}
DataPic= "https://publicdatarfu.blob.core.windows.net/publicdatarfucontainer/datenstruktur.PNG"
download.file(DataPic,'DataPic.png', mode = 'wb')
knitr::include_graphics("DataPic.png")
```

If we now have a look at the measured values itself, then we realise that a) there are a lot of values to deal with (51.3 millions) and b) the single values do not really have a "meaning" they are "just" single points on an axis. To get a clear and somehow meaningful set of features we have to bring this single values somehow in a context, either by analysing them in a time series, by its distribuition or by some statistical performance indicators. And this leads me over to the next chapter where we will see how we produced the final "Feature Vector" with which we then fed the different algorithms.

## Method/Analysis
### Generate the "Feature Vector"
So here I was with 9'120 single files and something over 50 million values which have to be transformed and merged into one single matrix, where each column is a meaningful feature-vector.
First I did not really have a clue how to takle this topic and as chances were low that I will get the master plan by thinking it through only I started with addressing just one of the topics to be solved: How to load the data from the single files into one R data object.
My initial idea was to load all raw data in a matrix per activity and all 19 matrixes were stored in an array.

Weiter mit: 
Schnell gemerkt, dass nix, aber viel gelernt - gestartet mit einem File, dann für eine Person, dann für eine Aktivität und dann für alles - verschachtelte if-Funktionen, Beschriftung, Überführung von txt. zu Matrix usw. 
Dann mit den Feature Umrechnung begonnen und eine neue If-Funktion gemacht, wo am Schluss eine grosse Matrix daraus entstand
Nun Zeitprobleme und wechel auf lapply super Zeiten, aber dann wechsel auf externe Datei und Zeit waren schlimm...
Dann wieder Probleme wegen dem zip. daher zuerst downloaden und dann von dort weiter
Jetzt zufrieden


### Train- and Test dataset



### Analyse train dataset and do feature reduction



### Train all six models and test accuracy with test dataset



## Analyse and discuss final results



## Personal Conclusion





The task is to establish a movie recommendation algorithm based on the MovieLens dataset.
This dataset consists of millions of movie ratings of thousands of movies from thousands of users.
The goal is it to predict the movie ratings with an "root-mean-square-error" (RMSE) below **0.86490**.
For this exercise we used the [10M MovieLens dataset](https://grouplens.org/datasets/movielens/10m/) version to make computation easier possible.
The calculations are done as approximations and not with a predefined algorithm like e.g. glm() function as this would have crashed my computer.
The approach was to build an algorithm following this logic: $Y_{i,u,c} = \mu +b_{i}+b_{u}+b_{?}+\epsilon_{i,u,c}$ where Y~i,u,c~ are the predicted ratings based on $\mu$, the movie bias (b~i~), the user bias (b~u~), the bias out of findings from Rating-Matrix decomposition (b~?~) and the non-further specified remaining error ($\epsilon$). To get to the final version of this algorithm (incl. final validation) I followed the following five steps:

1. Generate the edx dataset: Download and wrangle "10M Movilens" data. Split it into a dataset to work with - the "edx" dataset - and a validation dataset. This step was done by HarvardX for us.
2. Understand, wrangle and analyse edx dataset
3. Build a train- and test dataset out of edx dataset and define the RMSE function
4. Building the algorithm by a step by step approach:
    i) Just use $\mu+\epsilon$
    ii) add movie bias (b~i~) to the equation
    iii) add user bias (b~u~) to the equation
    iv) do improvements thanks to regularization
    v) Adding a bias, we calculate out of the decomposition of the Rating-Matrix
5. Validate algorithm against validation dataset and discuss result  

The knowledge/information used - beside different internet researches - is mainly from the material we learned in the module 8 of the HarvardX Data Science course series, recommenderlab documentation [^1], and two further publications one about singular value decomposition [^2] (SVD) and a tutorial about principal component analysis [^3] (PCA).
 
The RMSE value the algorithm achieved running against validation dataset is **0.86446** which means the final algorithm is good enough to achieve the goal set.
  
## Method/Analysis
### Generate edx dataset
The code to generate the edx dataset was provided by HarvardX. As there is only one minor change (added three additional libraries) it will not be commented further. In the .Rmd version of this document the code will be included.  

```{r starting code given by HarvardX, include=FALSE}
#####################################
# Create edx set, validation set ####
#####################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
#Add 2 libraries to the default code as we will need them later
if(!require(recommenderlab)) install.packages("recommenderlab", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("tinytex", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
  
  
### Understand, clean and analyse edx dataset

```{r myTheme}
# set settings for graphs:
myTheme <- theme(axis.title = element_text(size = 13),
  axis.text.x = element_text(size = 10), 
  axis.text.y = element_text(size = 10))
```

First we will have a look at edx dataset to get a "look and feel" for it:  

```{r analysis 1, echo=TRUE}
head(edx)
dim(edx)
class(edx)
```
  
So we have a `r class(edx)` with `r nrow(edx)` rows and `r ncol(edx)` columns, which is by the way composed of `r n_distinct(edx$movieId)` different movies and `r n_distinct(edx$userId)` different users.  
  
Next we will check if there are any zeros or "NA" in the dataset:  

```{r zero_na, echo=TRUE}
edx %>% filter(rating == 0) %>% tally()
sum(is.na(edx))
```
  
As everything looks quite good for this exercise no further data cleaning would be necessary. Nevertheless, it could be handy for future work to separate the movie title from the release year - so that we get a data frame with 7 columns. Even so  for better readability there is an adjustment of the column sequence. After these changes the new data.frame looks like this (new dataset name is "edx1"):  

```{r edx1, include=TRUE}
#Split title column in two columns: "title" and "year" and adjust classes of col.
edx1 <- edx %>% extract(title, c("title","year"), regex = "^(.*)\\s[(](\\d{4})[)]$")
edx1 <- edx1 %>% mutate(year = as.integer(year), genres = as.factor(genres))

#Better readability by bringing columns closer together which belong together
edx1 <- edx1[,c(2,5,6,7,1,3,4)]
head(edx1,5)
```
  
Now let us do some visualization to get more insights about the data:  

```{r gap rated movies, fig.cap= "Visualization of the Gaps in movie ratings shown by a random sample of 100 users"}
users <- sample(unique(edx1$userId), 100)
rafalib::mypar()
edx1 %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
abline(h=0:100+0.5, v=0:100+0.5, col = "grey")
```
  
```{r rated movies, fig.cap= "Distribution of amount of movie ratings"}
#First insights: Some movies get more rated then others:
edx1 %>% 
  dplyr::count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10() + 
  ggtitle("Movies") +
  labs(x = "Amount of ratings movies got", y="Amount of movies per 'amount of rating'-category") +
  myTheme
```
  
Figure 1 and 2 show in different ways the "gaps" we have in the ratings, meaning that by far not all movies got rated by all the users. This we must take into consideration when we build the algorithm as this could adulterate our results. For example, when ratings far away from the average are backed only from few users or when we want to work with a userId/movieId/rating-Matrix to do factorization.  
  
Concerning the ratings, it seems that people do prefer the integer values. Even so there is a clear tendency towards ratings above the scale average of 2.5 towards a rating between "above average to good" (3-4):  

```{r rated by user, fig.cap= "Distribution of ratings"}
#First insights: Some users are rating more than others:
edx1 %>% 
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.5, color = "black") + 
  ggtitle("Distribution of Ratings") +
  labs(x = "Rating scale", y="Amount of entries per Rating option") +
  myTheme
```
   
   
### Build a train- and test dataset + define RMSE function

With the `createDataPartition` function we build us two datasets out of the edx1 dataset. One is the edx_train set which we will use to train/build our algorithm. The other is edx_test against which we will test the prediction capabilities of our algorithm throughout the different steps. 

```{r building datasets, include=FALSE}
#Split edx1 dataset in a edx_train and edx_test set - use 15% of data for test set
set.seed(15, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = edx1$rating, times = 1, p = 0.15, list = FALSE)
edx_train <- edx1[-test_index,]
temp <- edx1[test_index,]

# Make sure userId and movieId in edx_test set are also in edx_train set
edx_test <- temp %>% 
  semi_join(edx_train, by = "movieId") %>%
  semi_join(edx_train, by = "userId")

# Add rows removed from edx_test set back into edx_train set
removed <- anti_join(temp, edx_test)
edx_train <- rbind(edx_train, removed)

#remove unnessecary objects
rm(test_index, temp, removed, edx1)
```
  
Following we look at the dimensions of the two datasets - we will see that edx_train set consists of about 85% of the original edx1 dataset and edx_test of 15% accordingly:  

```{r dim datasets, echo=TRUE}
dim(edx_train)
dim(edx_test)
```
  
Last but not least we define the RMSE function before we can start building the algorithm:  

```{r RMSE function, echo=TRUE}

RMSE <- function(predicted_ratings, true_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```
  
### Building the algorithm step by step

The algorithm will follow this logic: $Y_{i,u,c} = \mu +b_{i}+b_{u}+b_{c}+\epsilon_{i,u,c}$. We will build the algorithm in a step by step approach so we can compare results (RMSE against edx_test) after every step.  
#### step 1: Starting with "mu"
To start with we just predict the ratings by setting them on the rating average from edx_train $Y = \mu$:  

```{r calculate mu, echo=TRUE}
#Calculating mu 
mu <- mean(edx_train$rating)
mu
```
  
This results in the following RMSE value:  

```{r 1st RMSE table, include=TRUE}
#Calculate RMSE for 1st intermediate result based on mu:
Mu_rating <- edx_test %>% 
  mutate(pred = mu) %>%
  .$pred

RMSE_InterOne <- RMSE(Mu_rating, edx_test$rating)

#Visualize the RMSE result:
rmse_results <- data_frame(method="Mu", RMSE = RMSE_InterOne, Improvement = 0, )
rmse_results %>% knitr::kable()
```
 
#### Step 2: Adding b~i~
As a next step we add the movie bias (b~i~) to the algorithm. For this we take the average of each movie "corrected" by mu:  

```{r calculate bi, echo=TRUE}
bi_avg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu), n = n())
```
  
Including b~i~ RMSE gets the following value:  

```{r 2nd RMSE table, include=TRUE}
#Calculate RMSE for 2nd intermediate result:
MuBi_rating <- edx_test %>% 
  left_join(bi_avg, by='movieId') %>% 
  mutate(pred = mu + b_i) %>%
  .$pred

RMSE_InterTwo <- RMSE(MuBi_rating, edx_test$rating)

#Improvement:
Imp <- RMSE_InterOne-RMSE_InterTwo

#Visualize the RMSE result:
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Mu, b_i",
                                     RMSE = RMSE_InterTwo, Improvement = Imp ))
rmse_results %>% knitr::kable()
```
  
As we can see there is a substantial improvement of `r percent(Imp/RMSE_InterOne)` in the RMSE so the bias per movie seems to be somehow big. It seems by some movies people were +/- in agreement to give them a rating clearly above or below the overall rating average - as if a movie got a controversial rating (equally good and bad ratings) the deduction of mu would have "corrected" the effect.  
  
#### Step 3: Adding b~u~

Now we add the User bias (b~u~):  

```{r calculate bu, echo=TRUE}
bu_avg <- edx_train %>% 
  left_join(bi_avg, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
```

```{r 3rd RMSE table, include=TRUE}
#Calculate RMSE for 3rd intermediate result:
MuBiBu_rating <- edx_test %>% 
  left_join(bi_avg, by='movieId') %>% 
  left_join(bu_avg, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

RMSE_InterThree <- RMSE(MuBiBu_rating, edx_test$rating)

#Improvement:
Imp <- RMSE_InterTwo-RMSE_InterThree

#Visualize the RMSE result:
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Mu, b_i, b_u",
                                     RMSE = RMSE_InterThree, Improvement = Imp ))
rmse_results %>% knitr::kable()
```
  
Also adding b~u~ brought us a step forward. We achieved a further improvement of RMSE value by `r percent(Imp/RMSE_InterTwo)` (against edx_test dataset).  
  
#### Step 4: Adding regularization

As a next step we look at regularization. So, we want to penalize large deviations of the average caused from small sample sizes. But before doing the calculations we want to do a quick analysis of the data to check if such an effect could exist.  
To do so we want to have a special look at the residuals still bigger than |1| after deducting mu, b~i~ and b~u~:  

```{r show effect of small samples, include=TRUE, fig.cap= "Amount of ratings by movies with residuals > |1|"}
#prepare train set
temp <- edx_train %>% 
  left_join(bi_avg, by="movieId") %>%
  left_join(bu_avg, by='userId') %>%
  mutate(residual = rating - (mu+b_i+b_u)) 

#Preparation for later median calc. of total edx_train set:
Movie_Count <- edx_train %>% dplyr::count(movieId)

#Preparation for later median calc. of dataset with residuals > |1|
temp_2 <- filter(temp,abs(temp$residual)>1)
Movie_Count_s <- temp_2 %>% dplyr::count(movieId)

#Visualize the effect of low amount of movie-ratings and high residuals (scales package required)
temp_3 <- temp_2 %>% group_by(movieId) %>% summarize(n=n(), residual=mean(abs(residual))) %>% 
  arrange(desc(abs(residual)))

temp_3 %>% ggplot(aes(residual,n)) + 
  geom_point() + 
  scale_y_continuous(trans = log2_trans()) + 
  geom_line(aes(y=median(Movie_Count$n)), color="red") +
  geom_line(aes(y=median(Movie_Count_s$n)))+
  ggtitle("Distribution movie ratings (residuals > |1|)") +
  geom_text(aes(x = 3, y = 40,
                label = paste("Median of residuals > |1| =",paste(median(Movie_Count_s$n)))))+
  geom_text(aes(x = 3, y = 140 ,
                label = paste("Median of total edx_train set =",paste(median(Movie_Count$n))))) +
  myTheme
```
  
We see that the amount of ratings a movie got with residual still bigger |1| are clearly lower compared to the median of the whole training dataset (red line). Similar we can observe when we sort it by userId instead of movieId. Therefore, it is probably reasonable to do a regularization.  
First we calculate the optimal choice of lambda.  

```{r calculate lambda, include=TRUE, fig.cap= "RMSE development with differnt lambdas"}
lambdas <- seq(0, 15, 0.5)
rmses <- sapply(lambdas, function(lambda){
  bi_reg <- edx_train %>%
    group_by(movieId) %>%
    summarize(b_i_reg = sum(rating - mu)/(n()+lambda))
  bu_reg <- edx_train %>% 
    left_join(bi_reg, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - b_i_reg - mu)/(n()+lambda))
  predicted_ratings <- 
    edx_test %>% 
    left_join(bi_reg, by = "movieId") %>%
    left_join(bu_reg, by = "userId") %>%
    mutate(pred = mu + b_i_reg + b_u_reg) %>%
    .$pred
  return(RMSE(predicted_ratings, edx_test$rating))
})

qplot(lambdas, rmses)  

lambda <- lambdas[which.min(rmses)]
```
  
Now we recalculate the movie- and user bias "penalized" with optimal lambda (`r lambda`) - the new variables we will call b~i~_reg and b~u~_reg.  

```{r calculate b_i_reg and b_u_reg, echo=TRUE}
#Calculating b_i_reg based on best lambda value:
bi_reg_avg <- edx_train %>% 
  group_by(movieId) %>%
  summarize(b_i_reg = sum(rating - mu)/(n()+lambda))

#Calculating b_u_reg based on best lambda value:
bu_reg_avg <- edx_train %>% 
  left_join(bi_reg_avg, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u_reg = sum(rating - b_i_reg - mu)/(n()+lambda))
```
  
If we now generate RMSE based on the regularized biases we get the following values:  

```{r 4th RMSE table}
Reg_rating <- edx_test %>% 
  left_join(bi_reg_avg, by='movieId') %>% 
  left_join(bu_reg_avg, by='userId') %>%
  mutate(pred = mu + b_i_reg + b_u_reg) %>%
  .$pred

RMSE_InterFour <- RMSE(Reg_rating, edx_test$rating)

#Improvement:
Imp <- RMSE_InterThree-RMSE_InterFour

#Visualize the RMSE result:
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="'Regularized'",
                                     RMSE = RMSE_InterFour, Improvement = Imp ))
rmse_results %>% knitr::kable()
```
  
With a further improvement of RMSE score of `r percent(Imp/RMSE_InterThree)` this step did not bring us too much of an additional benefit. But it is also to say that we are at a stage with our algorithm where it gets harder and harder to find further improvements.

#### Step 5: Matrix decomposition

In this next step we want to try out two different approaches by decomposing the Rating-Matrix.  

1. Add a bias based on PCA. Thanks to this we should be able to consider dependency patterns in between movies and/or users in our algorithm. We call this bias "cluster bias" (b~c~)
2. With support of SVD we will rebuild the Rating-Matrix in a way that the Rating-Matrix is fully filled out (no empty cells) and then we take the mean of every movie. We will call this bias b~i2~

To anticipate one point already now: The initial idea was to do this in a sequence - so first calculate b~c~, include it in the algorithm and then based on this rebuild the Rating-Matrix and do step 2. But it showed that this does only bring the second-best result. The best result was achieved by just including one of the two new biases (b~c~, b~i2~) - which one we will see at the end of this chapter. 
  
##### PCA calculations (add b~c~)

To start with this step, we have to generate the Rating-Matrix. We will do this directly with a capability of the `recommenderlab` package and use the function `as(<data.frame>,realRatingMatrix)`. This function is converting a data.frame directly in a so called "real-value rating matrix", which is not only converting the data into a matrix but it will store it in a sparse format too (NA values will not be stored but zeros will). With this we can take into consideration too that not all movies got rated by all users and the matrix would therefore have a lot of "blanks" (as we found out in chapter "Understand, wrangle and analyse edx dataset").  

```{r Converet to realRatingMatrix, echo=TRUE}
#Calculate predictions for edx_train set:
Reg_rating_train <- edx_train %>% 
  left_join(bi_reg_avg, by='movieId') %>% 
  left_join(bu_reg_avg, by='userId') %>%
  mutate(pred = b_i_reg + b_u_reg) %>%
  .$pred

#Establish Rating-Matrix with Recommenderlab function - first deduct rating-predictions 
#from real ratings so only the residuals remain (not standardized yet):
PreMatrix_edx_train <- edx_train %>% 
  select(userId, movieId, rating) %>% 
  mutate(rating = rating - Reg_rating_train)
rRM <- as(PreMatrix_edx_train, "realRatingMatrix")
getRatingMatrix(rRM[1:10,1:10])
```
  
Unfortunately it was not possible to directly do PCA with the `prcomp()` function - it took extremely lot of time (over 12hours - yes, tested it...) and it generated huge datasets. By the way same applied for `svd()`.
So we will do all next step based on svd methodology and svd calculation cone by `recommender()` function from `recommenderlab`. To do so we first check the default settings of the function parameters:  

```{r Check model parameters}
#Store provided methods within recommender function in a variable:
recommender_models <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")

#Get parameter details from SVD method:
recommender_models$SVD_realRatingMatrix$parameter
```
  
These defaults settings should be good enough for us, so we generate singular value decomposition now:  
```{r generate SVD, echo=TRUE}
r_rRM <- Recommender(rRM, method = "SVD")
str(r_rRM@model$svd)
```
  
As a next step follows some math to reconstruct PCA elements. It is to say that the loadings-Matrix (in `prcomp()` function called "rotation") we do already have as it is the same as Matrix "V" from SVD:  

```{r calculate PCA components, echo=TRUE}
# 1. Step: Calculate covariance matrix of rRM via SDV parameters V and D:
cov_M <- sweep(r_rRM@model$svd$v, 2, r_rRM@model$svd$d^2, FUN="*") %*% 
  t(r_rRM@model$svd$v)/(ncol(rRM)-1)

# 2. step: Calculate Variation (sdev^2 value from prcomp() function):
eig_value <- sapply(i <- 1:ncol(r_rRM@model$svd$v),function(i) {
  (cov_M[i,]%*%r_rRM@model$svd$v[,i])/
    r_rRM@model$svd$v[i,i]})
sdev_calc <- sqrt(eig_value)

# 3. step: Calculate scores (x value from prcomp() function):
pcaScores <- getRatingMatrix(rRM) %*% r_rRM@model$svd$v
```
  
Now let's have a look at the variation parameters ($sdev\_calc^2$) to see what the influence on variation each principal component (PC) has:  

```{r visualize variation, fig.cap= "Variation per PC"}
barplot(eig_value/sum(eig_value), xlab = "PC", ylab = "Percent of total Variance explained", cex.names=0.8, ylim = c(0,1))
eig_value
```
  
As we see first PC is "dominating" clearly and so with just one PC we are capable to explain almost the whole variation. Therefore we will just consider the first column each of our loading- and scores matrix and only the first value of our variation vector to calculate b~c~ ($b_c = p*q$, while p= user effect and q = principal component):  

```{r calculate b_c, echo=TRUE}
b_c <- (pcaScores[,1]) %*% t(r_rRM@model$svd$v[,1]*eig_value[1])
b_c <- colMeans(b_c)
```
  
Now we add b~c~ in the same approach to the algorithm as we have done it before and get the following RMSE table:  

```{r 5th RMSE table}
#Build a data frame so that b_c can be added to edx_test set later: 
temp_factorization1 <- edx_train %>% group_by(movieId) %>% summarize(movieAverage = mean(rating)) %>% 
  cbind(b_c)

#Calculate RMSE for 5th intermediate result:
Fac_rating1 <- edx_test %>% 
  left_join(bi_reg_avg, by='movieId') %>%
  left_join(bu_reg_avg, by='userId') %>%
  left_join(temp_factorization1, by='movieId') %>%
  mutate(pred = mu + b_i_reg + b_u_reg + b_c) %>%
  .$pred

RMSE_InterFive <- RMSE(Fac_rating1, edx_test$rating)

#Improvement compared to after Regularization:
Imp1 <- RMSE_InterFour-RMSE_InterFive

#Visualize the RMSE result:
rmse_results <- bind_rows(rmse_results,
                          tibble(method="'PCA'",
                                     RMSE = RMSE_InterFive, Improvement = Imp1))
                        
rmse_results %>% knitr::kable()
```
  
##### SVD calculations (add b~i2~)
First, we will again have a look how much of variability is explained with each factor. In SVD we will use vector "D" for this. And again, the picture we gained from the PCA analysis that the first factor is explaining almost all the variation gets confirmed:  

```{r visualize variability, fig.cap= "'cumsum' of factor's variability"}
#How much variability is explained with first factor:
FirstFactor <- sum(r_rRM@model$svd$d[1]^2)/sum((r_rRM@model$svd$d)^2)

#Visualize how cumsum of each factor explains variability:
Variability <- cumsum(r_rRM@model$svd$d^2)/sum((r_rRM@model$svd$d)^2)
plot(c(0:10),append(0,Variability), type="b", xlab = "Factor", ylab="explained Variability",
     ylim = c(0,1), cex.lab=.8)
text(1,0.9, paste("Explained variability \nwith first factor",round(FirstFactor,3)), cex=.6)
```
  
Now we will reconstruct the Rating-Matrix and generate our next bias (b~i2~). To reconstruct the Rating-Matrix we will use the following formula:      
$$ Y = UDV^T $$
  
```{r calculate new Rating-Matrix, echo=TRUE}
#Calculate matrix Y with only with u,d,v of first factor:
resid <- with(r_rRM@model$svd,(u[, 1, drop=FALSE]*d[1]) %*% t(v[, 1, drop=FALSE]))
```
  
As we can see in the new Rating-Matrix there are no blanks left - in contrast to the initial Rating-Matrix shown above.  

```{r Show new Rating-Matrix}
resid[1:10, 1:5]
```

Next step is to generate b~i2~ by calculating the column means from our new Rating-Matrix:  

```{r calculate b_i2, echo=TRUE}
b_i2 <- colMeans(resid)
```

So, we have everything to calculate RMSE with b~i2~ added to the algorithm INSTEAD b~c~. Here we see the table with the final results (*Attention:* Improvement of "Full rating matrix" is measured against "Regularized" and not against "PCA"):  

```{r 6th RMSE table}
#Build a data frame so that b_i2 can be added to edx_test set later: 
temp_factorization2 <- edx_train %>% group_by(movieId) %>% summarize(movieAverage = mean(rating)) %>% 
  cbind(b_i2)

#Calculate RMSE for 5th intermediate result based on factorization: 

Fac_rating2 <- edx_test %>% 
  left_join(bi_reg_avg, by='movieId') %>%
  left_join(bu_reg_avg, by='userId') %>%
  left_join(temp_factorization2, by='movieId') %>%
  mutate(pred = mu + b_i_reg + b_u_reg + b_i2) %>%
  .$pred

RMSE_InterSix <- RMSE(Fac_rating2, edx_test$rating)

#Improvement compared to after Regularization:
Imp2 <- RMSE_InterFour-RMSE_InterSix 

#Visualize the RMSE result:
rmse_results <- bind_rows(rmse_results,
                          tibble(method= "'Full rating matrix'",
                                     RMSE = RMSE_InterSix, Improvement = Imp2))
                        
rmse_results %>% knitr::kable()
```
  
As we can see the better result we get when we add b~i2~ into our formula and therefore our final algorithm looks like this: $$Y_{i,u,c} = \mu +b_{i}+b_{u}+b_{i2}+\epsilon_{i,u,i2}$$

## Results
It is time to apply the generated algorithm to the validation set and check the results we get from it:  

```{r final result, echo=TRUE}
#Final step: Test the Algorithm against validation set:
Overall_rating <- validation %>% 
  left_join(bi_reg_avg, by='movieId') %>% 
  left_join(bu_reg_avg, by='userId') %>%
  left_join(temp_factorization2, by='movieId') %>%
  mutate(pred = mu + b_i_reg + b_u_reg + b_i2) %>%
  .$pred

RMSE_Overall <- RMSE(Overall_rating, validation$rating)

#Visualize the RMSE result:
rmse_final <- tibble(RMSE_Validation = RMSE_Overall)
rmse_final %>% knitr::kable()
```
  
With the final RMSE score of **`r RMSE_Overall`** we reached the goal to get a value below the expected 0.86490.  
Performance-wise it is certainly a result one can work (or at least start) with. But we also have to remain realistic then an average deviation of the effective rating of about 0.864 points is in our case almost as large as one "full rating category" (e.g. rating categories: very poor, poor, average, good, very good) - in today's time where people are already "spoiled" with quite exact recommendation predictions this would probably not be enough.  
  
## Personal conclusion
The chosen approach delivered the results we were looking for, which certainly is positive. Nevertheless the last step of the algorithm building process was not super satisfying as the PCA decomposition did not brought the results I had anticipated and therefore the efforts I put in it did not really pay off - at least I have now a slight idea about PCA and SVD.... But probably this is also part of a data scientist life: Sometimes you have to fail to get to the next level.
In general it is to say that with every additional bias we added to the algorithm it got tougher and tougher to generate additional benefits concerning the RMSE score. For me this is a hint that the chosen approach has its limitations somewhere around the RMSE score we achieved now. And if we want to bring it to a next level (let's say something like RMSE < 0.6) we would have to go for another approach.  
As potential future work I would see three different things I would like to try out:  

1. I would dig deeper into the last step of my algorithm building process - perhaps also by talking to a subject matter expert in SVD and PCA topics - to see if there is not more, I could get from it
2. It would be interesting to do this exercise once on a powerful "virtual machine" of a cloud solution (AWS, Azure...). With this extra compute- and memory power we could probably test out predefined algorithms like logistic regression or gradient boosting
3. Even though I do not have a clue about it now: It would be interesting to see the achievable results by applying deep learning algorithms - at least there should be enough data to give it a try with the huge MovieLens dataset

## References
K. Altun, B. Barshan, and O. Tunçel,
``Comparative study on classifying human activities with miniature inertial and magnetic sensors,''
Pattern Recognition, 43(10):3605-3620, October 2010
  
  
[^1]: https://archive.ics.uci.edu/ml/index.php
[^2]: https://www.kaggle.com/datasets
[^3]: https://archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities
[^4]: https://www.researchgate.net/publication/223300172_Comparative_study_on_classifying_human_activities_with_miniature_inertial_and_magnetic_sensors

[^3]: https://strata.uga.edu/software/pdf/pcaTutorial.pdf
[^4]: http://www2.uaem.mx/r-mirror/web/packages/recommenderlab/recommenderlab.pdf