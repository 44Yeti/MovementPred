#This script will generate a prediction algorithm for human movements based on the data generated by
#a magnetic sensor on each arm, each leg and the torso of the test person
#Attention: This script may run for quite a long time (on my Acer Aspire 5 laptop a bit more than 1 hour)

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org") #RColorBrewer included
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org") #helps by LaTeX documents
if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org") #sub-packages in use: utils, stats, graphics
if(!require(AppliedPredictiveModeling)) install.packages("AppliedPredictiveModeling", repos = "http://cran.us.r-project.org") #for graphics

#Libraries needed for model calculations:
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org") #Penalized Multinomial Regression
if(!require(klaR)) install.packages("klaR", repos = "http://cran.us.r-project.org") #naives Bayes
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org") #Gradient Boosting

###################### Defining Functions for later use ########################################
# Comment: An option could be to code these Functions in a separate script, but as I only have
# one function and as it is the "order" to have all in one script I did not do this.
################################################################################################
####################################################################################################################
#
Features <- function(f){
  # Maximum/Minimum, mean, variance, skewness, kurtosis per column
  f_min <- apply(f,2,min)
  f_max <- apply(f,2,max)
  f_mean <- colMeans(f)
  f_var <- colVars(f)
  f_skew <- apply(f,2,skewness)
  f_kurt <- apply(f,2,kurtosis)
  
  #----------Discrete Fourier Transform (with fast fourier transform)---------
  #Do Fast Fourier transform (fft)
  f_DFT_temp <- apply(f,2,fft)
  
  #Get the peaks and the phases
  f_DFT_Mod_temp <- apply(f_DFT_temp,2,Mod)
  
  #Choose just half of the values (seconde half is like a mirror of the first)
  f_DFT_Mod_temp_Half <- f_DFT_Mod_temp[1:(length(f_DFT_Mod_temp[,1])/2+1),]
  
  #Double all values beside for Frequency 0 (first index value)
  f_DFT_Mod_temp_Half[2:(length(f_DFT_Mod_temp[,1])/2),]<- f_DFT_Mod_temp_Half[2:(length(f_DFT_Mod_temp[,1])/2),]*2
  
  #Calculate Frequency:
  Freq <- seq(0,(25/2),length=length(f_DFT_Mod_temp[,1])/2+1)
  
  #Select the top 5 Peaks
  f_DFT_Mod <- apply(f_DFT_Mod_temp_Half,2,function(x){
    head(sort(x,decreasing = TRUE),5)
  })
  
  #Find corresponding Frequency
  for(i in 1:45) {
    
    if(exists("Index_vec")==FALSE){
      Index_vec <- match(f_DFT_Mod[,i],f_DFT_Mod_temp_Half[,i])}
    else{
      Index_vec <- c(Index_vec,match(f_DFT_Mod[,i],f_DFT_Mod_temp_Half[,i]))}
  }
  
  Index_matrix <- matrix(Index_vec, 5, 45)
  
  for(i in 1:45){
    if(exists("f_DFT_Freq")==FALSE){
      f_DFT_Freq <- Freq[Index_matrix[,i]]}
    else{
      f_DFT_Freq <- c(f_DFT_Freq,Freq[Index_matrix[,i]])}
  }
  #-------------------------------------- Autocorrelation -------------------
  #Get the Autocorrrelation for lag 50 (corresponds to 2 sek)
  f_Autoc_temp <- apply(f,2,function(x){
    acf(x, lag.max = 50, plot=FALSE)
  })
  
  #Get 11 values out of the 50 per "column" (here more "list section") - 
  #First value always =1 so not a lot of info from this, then I chose 4 correlation values
  #of the beginning, 4 values around 1 sek distance and 3 values at the end (=around 2sek difference)
  for(i in 1:45){
    if(exists("f_Autoc")==FALSE){
      f_Autoc <- f_Autoc_temp[[i]]$acf[c(2:5,23:26,48:50)]}
    else{
      f_Autoc <- c(f_Autoc,f_Autoc_temp[[i]]$acf[c(2:5,23:26,48:50)])}
  }
  #---------------------Building the Feature Vector ----------------------------------------------------------------------
  Feature_Vector <<- c(f_min,f_max,f_mean,f_var,f_skew,f_kurt,as.vector(f_DFT_Mod),f_DFT_Freq, f_Autoc) #global Variable "<<-"
}

########################## Create Feature_Matrix out of raw data #######################################################
####################################################################################################################
#
#--------------Prepare ColNames and the variables to extract the files (activites, person, files)---------------
ColNames <- c("T_xacc", "T_yacc", "T_zacc", "T_xgyro", "T_ygyro", "T_zgyro", "T_xmag", "T_ymag", "T_zmag",
              "RA_xacc", "RA_yacc", "RA_zacc", "RA_xgyro", "RA_ygyro", "RA_zgyro", "RA_xmag", "RA_ymag", "RA_zmag",
              "LA_xacc", "LA_yacc", "LA_zacc", "LA_xgyro", "LA_ygyro", "LA_zgyro", "LA_xmag", "LA_ymag", "LA_zmag",
              "RL_xacc", "RL_yacc", "RL_zacc", "RL_xgyro", "RL_ygyro", "RL_zgyro", "RL_xmag", "RL_ymag", "RL_zmag",
              "LL_xacc", "LL_yacc", "LL_zacc", "LL_xgyro", "LL_ygyro", "LL_zgyro", "LL_xmag", "LL_ymag", "LL_zmag")

#19 folders for the 19 activities:
activityNames <- c("a01","a02", "a03", "a04", "a05", "a06", "a07", "a08", "a09", "a10", "a11", "a12", "a13", "a14", "a15", 
                   "a16", "a17", "a18", "a19")

#8 person folders in each activity folder:
personNumber <- c("p1","p2","p3","p4","p5","p6","p7","p8")

#60 file names in each person folder:
fileNames <- sprintf("%02d",seq(1:60)) # gives me all numbers as 2 digits so that 1, 2, ... is "01", "02" etc


#----------------Downloading data in a temporary directory and unzip ------------------------------------------------
#Download the raw data
td <- tempdir()
tf = tempfile(tmpdir=td, fileext=".zip")
download.file("https://publicdatarfu.blob.core.windows.net/publicdatarfucontainer/data.zip", tf)
unzip(tf, exdir=td)
#I had "\\" as separater on the path which did not work on my windows system, to correct this I found the
#following correction option, which I hope will solve in a way that my code is also running on a non windows
#system without any problems. In worst case please adjust the separaters in fpath manually...
fpath <- normalizePath(td, winslash = "/", mustWork = NA)

#----------------- Excursus: Show some analysis done around DFT and Autocorrelation to understand the ----------------
#----------------- selection to be taken for the feature set (graphs to be shown in report)---------------------------
# DFT analysis for a single file:
filef <- paste0(fpath,"/data/a10/p1/s01.txt")
ff1 <- as.matrix(read_delim(filef, delim = ",", col_names = ColNames)) #ff2 --> Walking
filef <- paste0(fpath,"/data/a19/p1/s01.txt")
ff2 <- as.matrix(read_delim(filef, delim = ",", col_names = ColNames)) #ff2 --> Basketball

# Show symmetry in magnitude (without DC value)
ff1_DFT <- Mod(fft(ff1[,28]))
ff1_DFT[2:125] <- ff1_DFT[2:125]*2 #magnitudes have to be doubled (beside value for frequency 0)
Freq <- seq(1,25,length=124)
plot(Freq,ff1_DFT[2:125], main="Activity 'Walking': Acceleration sensor, x-axis, right leg", xlab = "Frequency", ylab= "Magnitude", 
     type = "l", cex.main=0.9, cex.lab=0.9)
abline(v=13, col="red")

#Show data when DC is included:
Freq <- seq(0,25,length=125)
plot(Freq,ff1_DFT[1:125], main="Activity 'Walking': Acceleration sensor, x-axis, right leg", xlab = "Frequency", ylab= "Magnitude", 
     type = "l", cex.main=0.9)
abline(v=12.5, col="red")

#Get the Autocorrrelation for lag 125 (corresponds to the full 5 sek)
Autoc1 <- acf(ff1[,28], lag.max = 125, main="Walking")
Autoc1 <- acf(ff2[,19], lag.max = 125, main="Basketball")
#----------------- End of the Excursus ----------------------------------------------------------------------------------
#
#----------------- Create feature vector by lapply loops ----------------------------------------------------------------

# Attention: Because of the read_delim function every loop generates a warning concerning the col_names and that it used the default.
# 
#Loop for the 19 activities
FV2 <- lapply(z <- activityNames, function(z){
  
  #Loop for the 8 person
  FV1 <- lapply(y <- personNumber, function(y){
    
    #Loop for the 60 time-series files
    FV <- lapply(x <- fileNames, function(x){
      file <- paste0(fpath,"/data/",z,"/",y,"/s",x,".txt")
      f <- as.matrix(read_delim(file, delim = ",", col_names = ColNames))
      if(exists("FV")==FALSE){
        FV <- vector("list",1)
        FV[[x]] <- Features(f)}
      else{
        FV[[x]] <- Features(f)}
    })
    
    if(exists("FV1")==FALSE){
      FV1 <- vector("list",1)
      FV1[[y]] <- Reduce(c,FV)}
    else{
      FV1[[y]] <- Reduce(c,FV)}
  })
  if(exists("FV2")==FALSE){
    FV2 <- vector("list",1)
    FV2[[z]] <- Reduce(c,FV1)}
  else{
    FV2[[z]] <- Reduce(c,FV1)}
})

Feature_Vector <- Reduce(c,FV2)

#--------------------Final step: Creat Feature_Matrix and clean-up---------------------------
#Prepare for the Naming of the columns and rows:
temp <- rep(ColNames,each=5)
temp1 <- c(rep(c("DFT_Mag_1_","DFT_Mag_2_","DFT_Mag_3_","DFT_Mag_4_","DFT_Mag_5_"),45))
temp_Mag <- paste0(temp1,temp)

temp1 <- c(rep(c("DFT_Freq_1_","DFT_Freq_2_","DFT_Freq_3_","DFT_Freq_4_","DFT_Freq_5_"),45))
temp_Freq <- paste0(temp1,temp)

temp <- rep(ColNames,each=11)
temp1 <- c(rep(c("Atuoc_1_","Atuoc_2_","Atuoc_3_","Atuoc_4_","Atuoc_5_","Atuoc_6_",
                 "Atuoc_7_","Atuoc_8_","Atuoc_9_","Atuoc_10_","Atuoc_11_"),45))
temp_Autoc <- paste0(temp1,temp)


DimNames_Col <- c(paste0("min_",ColNames),paste0("max_",ColNames),paste0("mean_",ColNames),paste0("var_",ColNames),
                  paste0("skew_",ColNames), paste0("kurt_",ColNames),temp_Mag,temp_Freq,temp_Autoc)


DimNames_Row <- c(rep("a01", (60*8)),rep("a02", (60*8)),rep("a03", (60*8)),rep("a04", (60*8)),rep("a05", (60*8)),
                  rep("a06", (60*8)),rep("a07", (60*8)),rep("a08", (60*8)),rep("a09", (60*8)),rep("a10", (60*8)),
                  rep("a11", (60*8)),rep("a12", (60*8)),rep("a13", (60*8)),rep("a14", (60*8)),rep("a15", (60*8)),
                  rep("a16", (60*8)),rep("a17", (60*8)),rep("a18", (60*8)),rep("a19", (60*8)))

#Generate a Matrix out of the Feature_Vector:
Feature_Matrix <- matrix(Feature_Vector,9120,1215,byrow = TRUE, dimnames = list(DimNames_Row, DimNames_Col))

#remove unnessecary objects
rm(ColNames,activityNames,personNumber,fileNames,ff1,ff2,ff1_DFT,Freq,filef,temp,temp1,temp_Freq,temp_Mag,temp_Autoc,FV2,Feature_Vector)

###################### Generate Train and Test Data ################################################################
####################################################################################################################
#
#Split Feature dataset in a Feature_train and Feature_test set - use 15% of data for test set
set.seed(2020) #so that I have reproducable results

test_index <- createDataPartition(y = rownames(Feature_Matrix), times = 1, p = 0.15, list = FALSE)
Feature_train <- Feature_Matrix[-test_index,]
Feature_test <- Feature_Matrix[test_index,]

#remove unnessecary object
rm(test_index)

#show dimensions of Matrix and ratio between train and test set
dim(Feature_train)
length(Feature_train[,1])/length(Feature_Matrix[,1])

######################## Analyse Data from Feature Set #############################################################
####################################################################################################################
#
#-----------------------------Check if everythink is "OK" with Feature_train- and _test dataset ------
#structure of Feature_train
str(Feature_train)
str(Feature_test)

#Are there any NA values?
sum(is.na(Feature_train))
sum(is.na(Feature_test))

#-----------------------------Comparison of activities sitting, walking, basketball ------------------ 
#-----------------------------based on acc. and gyro. sensors on RL and LA ---------------------------
#next steps will generate a vector with all the indexes for LA and RL for all axis of acc. and gyro sensor
#So that we can then "play around" with these data to compare them (it is easier like this as otherwise we
#always have to do the math at which position the searched values are...):
#
#left arm / acc x-axis
x1 <- c(19,64,109,154,199,244,361,586,919) #value 361 for Mag_1... value 362 for Mag_2
#right leg / acc x-axis
x2 <- c(28,73,118,163,208,253,(361+45),(586+45),(919+99)) #value 361 for Mag_1... value 362 for Mag_2
#left arm + right leg / acc x-axis
x <- sort(c(x1,x2))
x3 <- x
x4 <- x
x5 <- x
x6 <- x
x7 <- x
#left arm + right leg / acc y-axis
x3[1:12] <- x3[1:12]+1
x3[13:16] <- x3[13:16]+5
x3[17:18] <- x3[17:18]+11
#left arm + right leg / acc z-axis
x4[1:12] <- x4[1:12]+2
x4[13:16] <- x4[13:16]+10
x4[17:18] <- x4[17:18]+22
#left arm + right leg / gyro x-axis
x5[1:12] <- x5[1:12]+3
x5[13:16] <- x5[13:16]+15
x5[17:18] <- x5[17:18]+33
#left arm + right leg / gyro y-axis
x6[1:12] <- x6[1:12]+4
x6[13:16] <- x6[13:16]+20
x6[17:18] <- x6[17:18]+44
#left arm + right leg / gyro z-axis
x7[1:12] <- x7[1:12]+5
x7[13:16] <- x7[13:16]+25
x7[17:18] <- x7[17:18]+55

#final index vector for the feature selection
x8 <- sort(c(x1,x2,x3,x4,x5,x6,x7))

#select the rows for the activites a01, a10, a19:
activSelec <- c(1:408,3673:4080,7345:7752)

#Choise of comparison for the report:
#Compare a selection of acceleration sensor Features (magnitude and variance) of right leg sensor unit:
Compare1 <- Feature_train[activSelec,x8[c(43:45,79:81)]] #73:75
y <- as.factor(unique(rownames(Compare1)))

library(AppliedPredictiveModeling)
featurePlot(x = Compare1, 
            y = y, 
            plot = "box", 
            ## Pass in options to bwplot() 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(3,2), 
           auto.key = list(columns = 3),
           par.settings=list(box.rectangle=list(lwd=2)),
           par.strip.text = list(cex=.6, lines=2)
           )

#Now let's see the correlation matrix for these 3 activties and the selected features:
#activity 01 (sitting):
c1 <- cor(Feature_train[1:408,x8[c(43:45,79:81)]])
c1
#activity 10 (walking):
c2 <- cor(Feature_train[3673:4080,x8[c(43:45,79:81)]])
c2
#activity 19 (basketball):
c3 <- cor(Feature_train[7345:7752,x8[c(43:45,79:81)]])
c3

#Compare the results:
#Correlation of correclation values for Magnitude z-axis feature between sitting,walking and basketball:
tibble("Cor.a01/a10"=cor(c1[1:5,6],c2[1:5,6]), "Cor.a01/a19"=cor(c1[1:5,6],c3[1:5,6]), 
       "Cor. a10/a19"=cor(c2[1:5,6],c3[1:5,6])) %>% knitr::kable()
#the 3 means:
tibble("mean sitting"=mean(c1[1:5,6]),"mean walking"=mean(c2[1:5,6]), 
       "mean basketball"=mean(c3[1:5,6])) %>% knitr::kable()


########################## Feature Reduction #######################################################################
####################################################################################################################
#
#As a first step lets check how much of a reduction we get from "near zero variance" analysis:
nzv <- nearZeroVar(Feature_train)
length(nzv) #16 features
#remove these Features from Feature_train
Feature_train <- Feature_train[,-nzv]
#new dimension of Feature_train dataset:
dim(Feature_train)

#Find highly correlated predictors:
FeatureCor <-  cor(Feature_train)
summary(FeatureCor[upper.tri(FeatureCor)])
length(findCorrelation(FeatureCor, cutoff = 0.9)) #355 Features have a correlation >= 90% (3 Features >= 99%)

#Find linear dependencies:
findLinearCombos(Feature_train) #non to remove...

#------------------------ PCA with sacle.=FALSE (the one which will be used for the model training) -------------------
#PCA analysis with center = TRUE (default) but scale. = FALSE () - therefore PCA will be performed on 
#variance-covariance matrix and higher values will have more weight... (scale. = TRUE - correlation matrix) 
pca <- prcomp(Feature_train, scale. = FALSE)

#Rename the PCA elements for better understanding and use
variance <- (pca$sdev)^2
loading <- pca$rotation 
rownames(loading) <- colnames(Feature_train)  
scores <- pca$x

#Cool image but needs lots of time...
#d <- dist(Feature_Matrix)
#image(as.matrix(d), col = rev(RColorBrewer::brewer.pal(9, "RdBu")))

#Shows % of Variance-cumsum explained by PC
varPercent <- cumsum(variance/sum(variance) * 100)
barplot(varPercent[1:50], xlab='First 50 PC', ylab='Cumsum Variance %', ylim = c(0,100), 
        names.arg=1:length(varPercent[1:50]), col='black')
abline(h=99, col='red')
text(6,100,"99% of Variance",srt=0,pos=1, col = "red")

#Find # of PC which explain more dann 99% of Variance
PC_99 <- match("TRUE",cumsum(variance/sum(variance))>=0.99)-1
PC_99

#Show top loadings in PC1:
TopLoadings <- round(loading, 2)[,1:10]
Selection <- head(sort(abs(TopLoadings[,1]),decreasing = TRUE),8)
Selection

#-----------------------------------------------PCA with scale.=TRUE to compare---------------------------
#Just to compare: PCA with scale.=TRUE
pca1 <- prcomp(Feature_train, scale. = TRUE)

#Rename the PCA elements for better understanding and use
variance1 <- (pca1$sdev)^2
loading1 <- pca1$rotation 
rownames(loading1) <- colnames(Feature_train)  
scores1 <- pca1$x

#Shows % of Variance-cumsum explained by PC
varPercent1 <- cumsum(variance1/sum(variance1) * 100)
barplot(varPercent1[1:700], xlab='First 700 PC', ylab='Cumsum Variance %', ylim = c(0,100),
        names.arg=1:length(varPercent1[1:700]), col='gray')
abline(h=99, col='red')
text(90,100,"99% of Variance",srt=0,pos=1, col = "red")

#Find # of PC which explain more dann 99% of Variance
PC_991 <- match("TRUE",cumsum(variance1/sum(variance1))>=0.99)-1
PC_991

#Show top loadings in PC1:
TopLoadings1 <- round(loading1, 2)[,1:10]
Selection1 <- head(sort(abs(TopLoadings1[,1]),decreasing = TRUE),8)
Selection1

#-----------------------------------------Some visualitsations around PCA (scale.=FALSE)-------------------------
#do a scatter plot with the activities and show the loadings of the top 8 "influencer" of PC1
plot(scores[, 1], scores[, 2], xlab="PC 1", ylab="PC 2", 
     type="n", xlim=c(min(scores[, 1:2]), max(scores[, 1:2])), 
     ylim=c(-3000, 3000), cex.lab=0.9)
points(scores[, 1],scores[, 2], cex=.9, pch=20, col="grey65") # col=1:length(unique(rownames(scores[activSelec,]))))
scale <- 8000
arrows(0, 0, loading[names(Selection), 1]*scale, loading[names(Selection), 2]*scale, length=0.1, angle=20, col='red')
labelScale <- 1 
text(loading[names(Selection), 1]*scale*labelScale, loading[names(Selection), 2]*scale* labelScale, 
     rownames(loading[names(Selection),]), col='red', cex=0.6)

# Do a comparison of the 3 analysed activities in a PC1/PC2 relation
# (if we we want to compare only two activities here the ranges:
# 1:408 - a01
# 3673:4080 - a10
# 7345:7752 - a19)

#Plot the activities in relation to PC1 and PC2
data.frame(scores[activSelec,1:2], Activity=rownames(Feature_train[activSelec,])) %>% 
  ggplot(aes(PC1,PC2, fill = Activity))+
  geom_point(cex=2, pch=21) +
  scale_fill_manual(values=c("blue2", "red2", "forestgreen")) +
  coord_fixed(ratio = 1)

rm(x,x1,x2,x3,x4,x5,x6,x7,x8,activSelec, FeatureCor, pca1, PC_991,loading1, variance1, scores1, Selection1,
   TopLoadings1,c1,c2,c3)
################################# Train Models ####################################################################################
####################################################################################################################################
#
#-----------------Preparation: Bring pca_train/pca_test set into tibbles and define a "fitControl"-------
#Create pca_train and transform into tibble incl. Acitivites as factor
train_labels <- rownames(Feature_train)
pca_train <- scores[,1:PC_99]
pca_train <- pca_train %>% as_tibble() %>% cbind(train_labels,.) %>% rename(Activity = train_labels)
pca_train$Activity <- factor(pca_train$Activity)
pca_train[1:5,1:5]
class(pca_train[,"Activity"])

#Create pca_test and transform into tibble incl. Acitivites as factor
Feature_test <- Feature_test[,-nzv] #so that we have again the same Feature-Set as in train set
test_labels <- rownames(Feature_test)
pca_test <- sweep(Feature_test, 2, colMeans(Feature_test)) %*% loading
pca_test <- pca_test[,1:PC_99]
pca_test <- pca_test %>% as_tibble() %>% cbind(test_labels,.) %>% rename(Activity = test_labels)
pca_test$Activity <- factor(pca_test$Activity)

#k-Fold CrossValidation with 5 folds and 2 repetitions:
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 2,
  verboseIter = TRUE)

#----------------------- Penalized Multinomial Regression -------------------------------------------
set.seed(1978)

Fit_multi <- train(Activity ~ ., data = pca_train, 
                 method = "multinom", 
                 trControl = fitControl,
                 tuneGrid = data.frame(decay = c(0,0.0001)))
Fit_multi

predictions <- predict(Fit_multi, pca_test)
CM_multi <- confusionMatrix(predictions, pca_test$Activity)
CM_multi_Acc <- CM_multi$overall["Accuracy"]

#-----------------------k-Nearest Neighbours--------------------------------------------------
set.seed(1978)

Fit_knn <- train(Activity ~ ., data = pca_train, 
              method = "knn", 
              trControl = fitControl,
              tuneGrid = data.frame(k = c(5,7,9,11,15,20)))
Fit_knn

predictions <- predict(Fit_knn, pca_test)
CM_knn <- confusionMatrix(predictions, pca_test$Activity)
CM_knn_Acc <- CM_knn$overall["Accuracy"]

# Hint: done k-Nearest N. with PCA where scale. = TRUE - Accuracy against pca_test droped to 0.652, would
# need a deeper look in it.
# 
#-----------------------naive Bayes -- lots of warnings because auf too small probability values -------
set.seed(1978)

Fit_nBayes <- train(Activity ~ ., data = pca_train, 
                 method = "nb", 
                 trControl = fitControl,
                 tuneGrid = data.frame(fL = 0, usekernel = TRUE, adjust = 1))
Fit_nBayes

predictions <- predict(Fit_nBayes, pca_test)
CM_nBayes <- confusionMatrix(predictions, pca_test$Activity)
CM_nBayes_Acc <- CM_nBayes$overall["Accuracy"]

#-----------------------Support Vector Machines----------------------------------------------------
set.seed(1978)

Fit_svm <- train(Activity ~ ., data = pca_train, 
                 method = "svmLinear2",
                 trControl = fitControl,
                tuneGrid = data.frame(cost = c(0.5,1)))
Fit_svm

predictions <- predict(Fit_svm, pca_test)
CM_svm <- confusionMatrix(predictions, pca_test$Activity)
CM_svm_Acc <- CM_svm$overall["Accuracy"]

#-----------------------Gradient Boosting linear ----------------------------------------------------
set.seed(1978)

Fit_gb <- train(Activity ~ ., data = pca_train, 
                    method = "xgbLinear",
                    trControl = fitControl,
                    tuneGrid = data.frame(nrounds= 150, lambda=c(0,0.1), 
                                          alpha=0, eta=c(0.3,0.4)))
Fit_gb

predictions <- predict(Fit_gb, pca_test)
CM_gb <- confusionMatrix(predictions, pca_test$Activity)
CM_gb_Acc <- CM_gb$overall["Accuracy"]

#-----------------------Gradient Boosting Tree ----------------------------------------------------
set.seed(1978)

len=3
gbtGrid <-  expand.grid(max_depth = seq(2, len),
                         nrounds = floor((2:len) * 50),
                         eta = .3,
                         gamma = 0,
                         colsample_bytree = c(.6, .8),
                         min_child_weight = c(1),
                         subsample = seq(.5, .75, length = len-1))

Fit_gbt <- train(Activity ~ ., data = pca_train, 
                method = "xgbTree",
                trControl = fitControl,
                tuneGrid = gbtGrid)
Fit_gbt

predictions <- predict(Fit_gbt, pca_test)
CM_gbt <- confusionMatrix(predictions, pca_test$Activity)

CM_gbt_Acc <- CM_gbt$overall["Accuracy"]

#################################### Results #######################################################################
####################################################################################################################
#
#----------------------Compare all Accuracies----------------------------------
results <- tibble(method= c("Penalized Multinomial Regression", "k-Nearest Neighbours", "Naive Bayes",
                           "Support Vector Machines", "eXtreme Gradient Boosting (linear)", 
                           "eXtreme Gradient Boosting (Tree)"),
                                 Accuracy = c(CM_multi_Acc, CM_knn_Acc,
                                              CM_nBayes_Acc, CM_svm_Acc,
                                              CM_gb_Acc, CM_gbt_Acc))

results <- results[order(results$method),]
results <- results[order(results$Accuracy, decreasing = TRUE),]

results %>% knitr::kable()

#Deeper look at results by comparing the prediction/reference-matricies:
CM_gbt$table
CM_svm$table
CM_gb$table
CM_knn$table
CM_nBayes$table
CM_multi$table

#Overall statistics of the 4 top models:
CM_gbt$overall
CM_svm$overall
CM_gb$overall
CM_knn$overall

#Same amount of wrong predictions for knn and Gradient Boosting (linear):
sum(CM_gb$table[upper.tri(CM_gb$table)],CM_gb$table[lower.tri(CM_gb$table)])
sum(CM_gb$table[upper.tri(CM_knn$table)],CM_knn$table[lower.tri(CM_knn$table)])
